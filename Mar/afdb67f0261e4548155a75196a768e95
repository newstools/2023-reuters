LONDON, March 6 (Reuters) - Britain's opposition Labour Party has called on the government to enact its long-delayed Online Safety Bill, after Reuters revealed how few underage children Snapchat (SNAP.N) was removing from its platform. Britain, like the European Union and other countries, has been grappling with how to protect social media users, and in particular children, from harmful content without damaging free speech. On Friday, Reuters reported, Snapchat was kicking dozens of children in Britain off its platform each month, compared with tens of thousands blocked by rival TikTok. "Parents are crying out for better protections for children online and especially on social media," said Lucy Powell, Shadow Secretary of State for Digital, Culture, Media and Sport in a statement issued in response to Reuters' report. "The Government has delayed and now watered down the online safety bill, relying almost entirely on age verification technologies which aren't fool proof, when we know that many children pass themselves off as older online." Powell added that the British government needs to strengthen the Bill to "take on the algorithms and business models of platforms which promote harm and fail to protect children." Social media platforms require users to be at least 13 years old. These restrictions are intended to protect the privacy and safety of young children. The Online Safety Bill will require platforms to uphold the age limits and demonstrate how they are doing so, such as through age-verification technology. According to the data shared with media regulator Ofcom and seen by Reuters, between April 2021 and April 2022, TikTok blocked an average of around 180,000 suspected underage accounts in Britain every month, or around 2 million in that 12-month period. In the same period, Snapchat told the watchdog it had removed approximately 60 accounts per month, or just over 700 in total. “These figures are shocking but not surprising,” said Andy Burrows, a spokesperson for the Molly Rose Foundation, a child safety and suicide prevention organisation. Burrows said Snapchat needed to do more work to identify children on its platform, in order to better protect them from harmful content. “There is a perverse incentive for companies not to identify underage users, because it means not having to implement so many safeguards,” he said. “You need to know if you have children on your platform so you can offer them a safer experience. If you’re not rolling up your sleeves and doing the hard work, children aren’t getting any additional protection.” A Snap spokesperson told Reuters the figures misrepresented the scale of work the company did to keep under-13s off its platform. The spokesperson declined to provide additional context or to detail specific blocking measures the company has taken. "We take these obligations seriously and every month in the UK we block and delete tens of thousands of attempts from underage users to create a Snapchat account," the Snap spokesperson said. Our Standards: The Thomson Reuters Trust Principles.